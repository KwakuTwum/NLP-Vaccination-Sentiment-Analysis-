{"cells":[{"cell_type":"markdown","metadata":{"id":"lyZMFfSIQ8sT"},"source":["# Sentiment Analysis with Hugging Face"]},{"cell_type":"markdown","metadata":{"id":"e8Ha9X4sQ8sW"},"source":["Hugging Face is an open-source and platform provider of machine learning technologies. You can use install their package to access some interesting pre-built models to use them directly or to fine-tune (retrain it on your dataset leveraging the prior knowledge coming with the first training), then host your trained models on the platform, so that you may use them later on other devices and apps.\n","\n","Please, [go to the website and sign-in](https://huggingface.co/) to access all the features of the platform.\n","\n","[Read more about Text classification with Hugging Face](https://huggingface.co/tasks/text-classification)\n","\n","The Hugging face models are Deep Learning based, so will need a lot of computational GPU power to train them. Please use [Colab](https://colab.research.google.com/) to do it, or your other GPU cloud provider, or a local machine having NVIDIA GPU."]},{"cell_type":"code","execution_count":1,"metadata":{"scrolled":false,"id":"ttX6ld4_Q8sY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694962429248,"user_tz":0,"elapsed":46829,"user":{"displayName":"Kwaku T-Ampofo","userId":"11700430479663096887"}},"outputId":"0a630fbc-0329-4466-aa57-867cd933a93b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting transformers\n","  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n","Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.33.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Collecting accelerate>=0.20.3 (from transformers[torch])\n","  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m857.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.23.0\n"]}],"source":["# Install required libraries\n","%pip install torch\n","%pip install transformers\n","%pip install datasets\n","%pip install transformers[torch]"]},{"cell_type":"markdown","metadata":{"id":"Gp0aLMTVQ8sZ"},"source":["## Application of Hugging Face Text classification model Fune-tuning"]},{"cell_type":"markdown","metadata":{"id":"0CxL6MzqQ8sa"},"source":["Find below a simple example, with just `3 epochs of fine-tuning`.\n","\n","Read more about the fine-tuning concept : [here](https://deeplizard.com/learn/video/5T-iXNNiwIs#:~:text=Fine%2Dtuning%20is%20a%20way,perform%20a%20second%20similar%20task.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoPW1pm7Q8sa"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset\n","from datasets import load_metric\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","from transformers import TrainingArguments\n","from transformers import AutoModelForSequenceClassification\n","from transformers import DataCollatorWithPadding\n","from transformers import Trainer\n","from huggingface_hub import notebook_login\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFKd2VQ6Q8sb"},"outputs":[],"source":["# Disabe W&B\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"797-TtovQ8sb"},"outputs":[],"source":["# Load the train dataset\n","train_df = pd.read_csv('/content/drive/MyDrive/NLP-Vaccination-Sentiment-Analysis LP5/data/Train.csv')\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAeyUMZkQ8sd"},"outputs":[],"source":["# Load the test dataset\n","test_df = pd.read_csv('/content/drive/MyDrive/NLP-Vaccination-Sentiment-Analysis LP5/data/Test.csv')\n","test_df.head()"]},{"cell_type":"markdown","metadata":{"id":"ouBulVd8Q8se"},"source":["# Exploratory Data Analysis: EDA"]},{"cell_type":"markdown","metadata":{"id":"GY-KnTW8Q8se"},"source":["### Column Information of The Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IL6cbXY-Q8sf"},"outputs":[],"source":["train_df.info()"]},{"cell_type":"markdown","metadata":{"id":"4de0UZgUQ8sf"},"source":["- The train DataFrame contains a total of 10,001 entries (rows), and four columns: 'tweet_id', 'safe_text', 'label', and 'agreement'.\n","- The 'tweet_id' and 'safe_text' columns are of type 'object', typically representing strings. The 'label' and 'agreement' columns are of type 'float64', indicating floating-point numbers.\n","- The 'label' and 'agreement' columns contain missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Snfxj5kTQ8sf"},"outputs":[],"source":["test_df.info()"]},{"cell_type":"markdown","metadata":{"id":"_2MhCh4NQ8sg"},"source":["- The test DataFrame consists of 5,177 entries (rows), and two columns: 'tweet_id' and 'safe_text'.\n","- Both columns, 'tweet_id' and 'safe_text', are of type 'object', representing strings.\n","- The 'safe_text' column has a missing value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L89NgSheQ8sg"},"outputs":[],"source":["# Checking the shape of the train dataset\n","train_df.shape"]},{"cell_type":"markdown","metadata":{"id":"suroZUpsQ8sh"},"source":["The train dataset as earlier mentioned has 1001 rows and 4 columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cC__6Py-Q8sh"},"outputs":[],"source":["# Checking the shape of the train dataset\n","test_df.shape"]},{"cell_type":"markdown","metadata":{"id":"yJcCYObdQ8si"},"source":["The test dataset has 5177 rows and 2 columns. The dataset lacks the 'label' and the 'safe_test' columns."]},{"cell_type":"markdown","metadata":{"id":"nmkyY4mDQ8si"},"source":["### Checking for and Handling Missing Values"]},{"cell_type":"markdown","metadata":{"id":"0biS6SppQ8si"},"source":["#### Train Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cnJs-wYQ8si"},"outputs":[],"source":["# Checking for missing values in the train dataset\n","train_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6YdrjulQ8sj"},"outputs":[],"source":["# Drop The Rows With Missing Data\n","train_df = train_df.dropna()\n","\n","# Confirm if the missing values have been handled\n","train_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"cQm7ct6fQ8sj"},"source":["#### Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUsHBz0YQ8sj"},"outputs":[],"source":["# Checking for missing values in the test dataset\n","test_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvYy2lpTQ8sk"},"outputs":[],"source":["# Drop The Rows With Missing Data\n","test_df = test_df.dropna()\n","\n","# Confirm if missing values have been handled\n","test_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"nSghj5jEQ8sk"},"source":["All missing Values in the Train and Test Datasets have been handled."]},{"cell_type":"markdown","metadata":{"id":"LLWzRZkuQ8sk"},"source":["### Visualizing The Distribution of Data In The Different Datasets"]},{"cell_type":"markdown","metadata":{"id":"qVeSL10yQ8sk"},"source":["#### i. Distribution of The 'label' column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aY4w1UAtQ8sk"},"outputs":[],"source":["# Replace numeric sentiment labels with descriptive words\n","label_mapping = {-1: 'Negative', 0: 'Neutral', 1: 'Positive'}\n","train_df['label'] = train_df['label'].map(label_mapping)\n","\n","# Class Distribution in 'label' column with 'viridis' color palette\n","plt.figure(figsize=(8, 6))\n","sns.countplot(data=train_df, x='label', palette='viridis')\n","plt.title('Lalel Distribution in Train Dataset')\n","plt.xlabel('Sentiment Labels')\n","plt.ylabel('Count')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BU8nrXLyQ8sl"},"source":["The plot visualizes the distribution of sentiment labels ('Negative,' 'Neutral,' 'Positive') in the training dataset.\n","From the plot, we can observe the following insights:\n","- The 'Neutral' sentiment appears to be the most frequent category, followed by 'Positive,' and 'Negative' sentiments.\n","- The sentiment classes seem to be relatively balanced, with 'Neutral' having the highest count and 'Negative' having the lowest count.\n","- This balance suggests that the dataset does not suffer from severe class imbalance issues, which is beneficial for model training."]},{"cell_type":"markdown","metadata":{"id":"V-bYaxjjQ8sl"},"source":["### ii. Exploration of Text Data (Tweet_Length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgKrDvgCQ8sl"},"outputs":[],"source":["# Calculate and visualize the length of tweets\n","train_df['tweet_length'] = train_df['safe_text'].apply(len)\n","\n","plt.figure(figsize=(8, 6))\n","sns.histplot(train_df['tweet_length'], bins=50, palette='viridis')\n","plt.title(\"Distribution of Tweet Length (Character Count)\")\n","plt.xlabel(\"Tweet Length\")\n","plt.ylabel(\"Count\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"l-kZEhROQ8sm"},"source":["The plot explores the distribution of tweet lengths in terms of character count. Insights from this plot include:\n","- The distribution is left-skewed, with most tweets having relatively longer character counts.\n","- There is a peak in the distribution around the 100-150 character mark.\n","- This suggests that a substantial portion of tweets related to vaccination topics is relatively concise, but there is also a presence of shorter tweets.\n","- These insights can be valuable for further data preprocessing and model development. Understanding the distribution of tweet lengths can help in deciding the appropriate text preprocessing techniques and model architectures for handling different text lengths effectively.For instance, models should be capable of handling both longer and shorter texts effectively, considering the variability in tweet lengths within the dataset."]},{"cell_type":"markdown","metadata":{"id":"ikc_-LKpQ8sm"},"source":["### iii. Exploration of Text Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttt8jKpQQ8sm"},"outputs":[],"source":["# Plot the distribution of agreement\n","plt.figure(figsize=(8, 6))\n","sns.histplot(train_df['agreement'], bins=30)\n","plt.title(\"Distribution of Agreement\")\n","plt.xlabel(\"Agreement Percentage\")\n","plt.ylabel(\"Count\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pzSk9zUFQ8sn"},"source":["The plot visualizes the distribution of the agreement percentages in the training dataset. The agreement percentage indicates the percentage of reviewers who agreed on the sentiment label for a given tweet.\n","Key insights from this plot:\n","- Many tweets in the dataset have a consensus among reviewers regarding their sentiment labels.\n","- However, there is also a presence of tweets with lower agreement percentages, indicating potential disagreement among reviewers on sentiment labeling."]},{"cell_type":"markdown","metadata":{"id":"lSIf6IKuQ8sn"},"source":["### Visualize Word Cloud From  The Text Contained in The Tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7xZp_LJQ8sn"},"outputs":[],"source":["# Generate a word cloud from the tweet text\n","wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(train_df['safe_text']))\n","\n","plt.figure(figsize=(11, 10))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.title(\"Word Cloud of Tweet Text\")\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jP8-hUeeQ8s1"},"source":["The plot presents a word cloud generated from the tweet text in the training dataset.\n","Insights from this word cloud:\n","- The most frequent words in the tweet text are displayed in larger fonts, making them easily identifiable.\n","- Common words related to vaccination topics, and public health such as vaccine, vaccinate, vaccination are more prominent in the word cloud.\n","- This visualization provides a qualitative view of the textual content in the dataset, highlighting the importance of certain words or phrases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vd7Z9pKhQ8s1"},"outputs":[],"source":["# Extract hashtags from the 'safe_text' column\n","hashtags = train_df['safe_text'].str.findall(r'#\\w+')\n","\n","# Flatten the list of lists into a single list of hashtags\n","hashtags = [item for sublist in hashtags for item in sublist]\n","\n","# Count the frequency of each hashtag\n","hashtag_freq = Counter(hashtags)\n","\n","# Get the top 10 hashtags\n","top_10_hashtags = dict(hashtag_freq.most_common(10))\n","\n","# Create a bar plot to visualize the top 10 hashtags with the 'viridis' color palette\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=list(top_10_hashtags.keys()), y=list(top_10_hashtags.values()), palette='viridis')\n","plt.xlabel('Hashtags')\n","plt.ylabel('Frequency')\n","plt.title('Top 10 Hashtags')\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Oq7JGe40Q8s2"},"source":["The bar plot reveals the top 10 hashtags that are most frequently used in the Twitter posts within the dataset\n","Each hashtag represents a topic or theme discussed in the Twitter posts. By examining the top hashtags, we identify the most prevalent topics of discussion among Twitter users."]},{"cell_type":"markdown","metadata":{"id":"lki_t8vdQ8s2"},"source":["# Splitting the Train Dataset Into Train and Evaluation Subsets"]},{"cell_type":"markdown","metadata":{"id":"CDJhjwjZQ8s2"},"source":["I manually split the training set to have a training subset ( a dataset the model will learn on), and an evaluation subset ( a dataset the model with use to compute metric scores to help use to avoid some training problems like [the overfitting](https://www.ibm.com/cloud/learn/overfitting) one ).\n","\n","There are multiple ways to do split the dataset. You'll see two commented line showing you another one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-EZxjAsQ8s3"},"outputs":[],"source":["# Split the train data => {train, eval}\n","train, eval = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stlheXcwQ8s3"},"outputs":[],"source":["train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oi-VJEdQQ8s4"},"outputs":[],"source":["eval.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3N5zleOAQ8s4"},"outputs":[],"source":["print(f\"The new dataframe shapes: train is {train.shape}, eval is {eval.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1M0cOlOlQ8s4"},"outputs":[],"source":["# Save splitted subsets\n","train.to_csv(\"/content/drive/MyDrive/NLP-Vaccination-Sentiment-Analysis LP5/data/train_subset.csv\", index=False)\n","eval.to_csv(\"/content/drive/MyDrive/NLP-Vaccination-Sentiment-Analysis LP5/data/eval_subset.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a12qyH11Q8s5"},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset('csv',\n","                        data_files={'train': '/content/drive/MyDrive/NLP-Vaccination-Sentiment-Analysis LP5/data/train_subset.csv',\n","                        'eval': '/content/drive/MyDrive/NLP-Vaccination-Sentiment-Analysis LP5/data/eval_subset.csv'}, encoding = \"ISO-8859-1\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-i47xjaQ8s5"},"outputs":[],"source":["# Import the Tokenizer for RoBERTa\n","tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels=3)"]},{"cell_type":"markdown","metadata":{"id":"B_Nu0P-bQ8s6"},"source":["Tokenization is the process of converting a sequence of text, such as a sentence or a document, into individual numerical units or tokens. In the context of natural language processing (NLP), these tokens are typically words or subwords. Tokenization is a crucial step in preparing text data for analysis or machine learning tasks.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlvDojSHQ8s6"},"outputs":[],"source":["# Function for transforming the labels\n","def transform_labels(label):\n","    label = label['label']\n","    num = 0\n","    if label == 'Negative': # -1\n","        num = 0\n","    elif label == 'Neutral': # 0\n","        num = 1\n","    elif label == 'Positive': # 1\n","        num = 2\n","\n","    return {'labels': num}\n","\n","# Function to tokenize the text data\n","def tokenize_data(example):\n","    return tokenizer(\n","        example['safe_text'],\n","        padding='max_length',\n","        truncation=True,\n","        max_length=256\n","    )\n","\n","# Apply tokenization to the dataset, adding 'input_ids' and 'attention_mask' columns\n","dataset = dataset.map(tokenize_data, batched=True)\n","\n","# Transform labels to numerical values and remove unnecessary columns\n","remove_columns = ['tweet_id', 'label', 'safe_text', 'agreement']\n","dataset = dataset.map(transform_labels, remove_columns=remove_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjZgUdjhQ8s6"},"outputs":[],"source":["# View the contents and structure of the processed train and test datasets\n","dataset"]},{"cell_type":"code","source":["# Access labels for the train dataset\n","train_labels = dataset['train']['labels']\n","\n","# Access labels for the eval dataset\n","eval_labels = dataset['eval']['labels']\n","\n","# Print the first few labels for reference\n","print(\"Train Labels:\", train_labels[:20])\n","print(\"Eval Labels:\", eval_labels[:20])"],"metadata":{"id":"2o2QrslesVOw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The labels in both datasets have been properly tokenized."],"metadata":{"id":"zFGQNXy-sis7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"spz09b_CQ8s7"},"outputs":[],"source":["# Configure the training parameters using TrainingArguments\n","training_args = TrainingArguments(\n","\n","    # Directory where model checkpoints and logs will be saved\n","    \"covid-19_tweets_sentiment_analysis_RoBERTa_model\",\n","\n","    # Number of training epochs (iterations over the dataset)\n","    num_train_epochs=5,\n","\n","    # Load the best model at the end of training\n","    load_best_model_at_end=True,\n","\n","    # Define evaluation strategy (evaluate at specified epochs)\n","    evaluation_strategy=\"epoch\",\n","\n","    # Define how often to save model checkpoints\n","    save_strategy=\"epoch\",\n","\n","    # Directory for logs\n","    logging_dir=\"./logs\",\n","\n","    # Log every N steps\n","    logging_steps=500\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"ExDIMkXvQ8s8"},"outputs":[],"source":["# Loading the pretrained RoBERTa model while specifying the number of labels in our dataset for fine-tuning\n","model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels=3)"]},{"cell_type":"code","source":["# Define evaluation metrics\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"],"metadata":{"id":"LVfyuRD7eBIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJmvLOFIQ8s8"},"outputs":[],"source":["# Instantiate the training and evaluation sets\n","train_dataset = dataset['train'].shuffle(seed=100)\n","eval_dataset = dataset['eval'].shuffle(seed=100)"]},{"cell_type":"markdown","source":["In this code, we are creating training and evaluation datasets from our original dataset. We shuffle the data using a random seed of 100. Shuffling is a common practice in training machine learning models to ensure that the data samples are presented to the model in a random order during each epoch. This helps prevent any potential bias in the training process."],"metadata":{"id":"SfdClqbnMfZW"}},{"cell_type":"code","source":["# Converting training data to PyTorch tensors and adding padding for improved training efficiency:\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"vXEKYvn8V_Hv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code prepares a data collator that will convert the training data into PyTorch tensors and add padding to the sequences. This is done to optimize the training process, making it more efficient and faster. Padding ensures that all sequences have the same length, which is important for efficient batch processing during training."],"metadata":{"id":"FJxl0sHlek3T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlDINdUYQ8s9"},"outputs":[],"source":["# Instantiate the trainer\n","trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset,\n","    tokenizer = tokenizer,\n","    data_collator = data_collator,\n","    compute_metrics = compute_metrics\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7r3fMciQ8s9"},"outputs":[],"source":["# Launch the learning process: training\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7oUr82qQ8s_"},"outputs":[],"source":["# Reinstantiate the trainer for evaluation\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_ssqc37Q8s_"},"outputs":[],"source":["# Launch the final evaluation\n","trainer.evaluate()"]},{"cell_type":"code","source":["# Login to HuggingFace Hub\n","notebook_login()"],"metadata":{"id":"mFj4pNYYRSaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Push the fine-tuned model and tokenizer to HuggingFace Hub\n","model.push_to_hub(\"rasmodev/Covid-19_Sentiment_Analysis_RoBERTa_Model\", create_pr=1)\n","tokenizer.push_to_hub(\"rasmodev/Covid-19_Sentiment_Analysis_RoBERTa_Model\", create_pr=1)"],"metadata":{"id":"VLpF1JerXS0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SkF7XrK-dpNV"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"1ab24538aa0da4b2d8c48eaca591ff7ffc54671225fb0511b432fd9e26a098ba"}},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}